# ğŸ“˜ Day 01 2025.4.1 - æ–‡æœ¬é¢„å¤„ç†æ¨¡å— & è¯é¢‘ç»Ÿè®¡å™¨

## âœ… ä»Šæ—¥å®Œæˆä»»åŠ¡

- [x] ç¼–å†™æ–‡æœ¬é¢„å¤„ç†æ¨¡å—ï¼ˆ`preprocess_utils.py`ï¼‰
  - clean_text(): å°å†™åŒ– + å»æ ‡ç‚¹
  - tokenize(): ä½¿ç”¨ nltk åˆ†è¯
  - remove_stopwords(): å»è‹±æ–‡åœç”¨è¯
- [x] ç¼–å†™æµ‹è¯•è„šæœ¬ï¼ˆ`test_preprocess.py`ï¼‰å¹¶æˆåŠŸè¿è¡Œ
- [x] ç¼–å†™è¯é¢‘ç»Ÿè®¡å™¨ï¼ˆ`word_count.py`ï¼‰
- [x] å®Œæˆå­¦ä¹ æ€»ç»“ README æ–‡æ¡£
- [x] æˆåŠŸæäº¤ GitHub ğŸ‰

---

## ğŸ§  æ¨¡å—è¯´æ˜

### `preprocess_utils.py`
> å°è£…äº†è‹±æ–‡æ–‡æœ¬é¢„å¤„ç†å¸¸ç”¨å‡½æ•°ï¼Œå¯åœ¨æœªæ¥é¡¹ç›®ä¸­ç›´æ¥å¤ç”¨ï¼š

```python
clean_text(text) -> str
tokenize(text) -> list[str]
remove_stopwords(tokens) -> list[str]
